#!/cluster/home/een023/.virtualenvs/p3/bin/python

"""Generate aggregated netCDF files.

Send in a path to .nc files and the name of the files. Either as a list of many files or
with the asterisk (wildcard) notation, `"*.nc"`. Note that when using the wildcard
notation the string must be wrapped in quotation marks like in the above example.

Then creates a summary file for temperature or any valid attribute.
Usage:
    gen_temp -p look/here -i one.nc two.nc -sp input -o output_name -y -a ATTR
"""


import argparse
import datetime
import glob
import os
import sys
from typing import List, Union

import xarray as xr

parser = argparse.ArgumentParser(
    description="Create a file containing only the temperature variable.",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
)
parser.add_argument("-p", "--path", type=str, default=".", help="path to .nc files")
parser.add_argument(
    "-sp",
    "--savepath",
    type=str,
    default="input",
    help="path to where the .nc file is saved",
)
parser.add_argument(
    "-i",
    "--input",
    type=str,
    nargs="+",
    help='input .nc files. Use quotes around *, e.g. "*.nc".',
)
parser.add_argument("-o", "--output", help="output .nc files")
parser.add_argument(
    "-y",
    "--year",
    action="store_true",
    help="If given, a 12 month running average is computed.",
)
parser.add_argument(
    "-a",
    "--attributes",
    type=str,
    nargs="+",
    default=["TREFHT"],
    help="List of attributes of netCDF file",
)
parser.add_argument(
    "--append-to",
    type=str,
    default="",
    help="An output file that should be appended to with the input data files.",
)

args = parser.parse_args()
if args.append_to != "":
    sys.exit(
        "Sorry, but an appending method will not be implemented in the near future."
        " Instead you can use the ncrcat function from the NCO family"
        " (http://nco.sf.net/nco.html#ncrcat)."
    )
# Correct the input argument
if args.input is None:
    raise ValueError("you must give the input files")
# Correct the output argument
if args.output is None:
    output = datetime.date.today().strftime("%Y%m%d")
else:
    output = args.output
if output.split(".")[-1] != "nc":
    output += ".nc"
# Correct the path argument
if args.path is not None:
    path = f"{args.path}/" if args.path[-1] != "/" else args.path
else:
    path = ""
# Combine the path with all files
inputs = [
    (
        f"{path}{file}"
        if file.split(".")[-1] == "nc" or "*" in file
        else f"{path}{file}.nc"
    )
    for file in args.input
]
# If an asterisk (*) is used, all other files are discarded
the_input: Union[str, List[str]]
if any("*" in f for f in inputs):
    val = inputs[[i for i, s in enumerate(inputs) if "*" in s][0]]
    the_input = str(val)
    if not glob.glob(the_input):
        print(f"I could not find {the_input}")
        print("Exiting...")
        sys.exit()
else:
    the_input = inputs
    for input_ in the_input:
        if not glob.glob(input_):
            print(f"I could not find {input_}")
            sys.exit("Exiting...")


def _attr_present(attr) -> bool:
    # If the_input is a list, choose the first, otherwise glob expand and then choose
    # the first.
    if isinstance(the_input, str):
        check_input = glob.glob(the_input)[0]
    elif isinstance(the_input, list):
        check_input = the_input[0]
    ds = xr.open_mfdataset(check_input)
    try:
        _ = getattr(ds, attr)
    except AttributeError as e:
        print(e)
        return False
    else:
        return True
    finally:
        ds.close()


# Correct the savepath argument
savepath = args.savepath if args.savepath is not None else ""
savepath = path if savepath == "input" else savepath
savepath = f"{savepath}/" if savepath != "" and savepath[-1] != "/" else savepath
attrs = []
for a in args.attributes:
    op = a + output
    # Check if output file exist
    if os.path.exists(savepath + op):
        print(
            f"Seems like a file with the name {op} already exist.",
            "Come back tomorrow, or give a custom name using the '-o' option.",
        )
    elif _attr_present(a):
        if savepath != "":
            os.makedirs(savepath, exist_ok=True)
        print("Saving to", savepath + op)
        attrs.append(a)
if not attrs:
    sys.exit("All attributes files already exist. Exiting...")

print("Creating aggregated dataset... ", end="", flush=True)
dataset = xr.open_mfdataset(the_input)
dataset = xr.decode_cf(dataset)
print("Finished creating aggregated dataset.")
for i, a in enumerate(attrs):
    print(
        f"{i+1}/{len(attrs)}: Start creating file for attr {a}... ", end="", flush=True
    )
    try:
        ds = getattr(dataset, a)
    except AttributeError as e:
        print(f"\t{e}")
    else:
        if args.year:
            ds = ds.chunk({"time": 12})
            r = ds.rolling(time=12)
            ds = r.mean()
        ten_years = 3 * 365  # Found this just by trial & error...
        tabs = "\t"
        if len(ds.dims) > 3 and len(ds.time.data) > ten_years:
            tot_length = len(ds.time.data)
            parts = 1
            while parts * ten_years < tot_length:
                if os.path.exists(savepath + a + output[:-3] + f"-{parts}.nc"):
                    print(f"{tabs}Part {parts} already exists, skipping...")
                    tabs = "\t" * 5 + "\t" * int(len(a) // 8)
                    parts += 1
                    continue
                bulk = ds[(parts - 1) * ten_years : parts * ten_years]
                bulk = ds.to_dataset()
                bulk.attrs[
                    "history"
                ] = f"Time span: From {ds.time.data[0]} to {ds.time.data[-1]}"
                bulk.to_netcdf(
                    savepath + a + output[:-3] + f"-{parts}.nc", unlimited_dims="time"
                )
                bulk.close()
                parts += 1
        else:
            ds = ds.to_dataset()
            ds.attrs[
                "history"
            ] = f"Time span: From {ds.time.data[0]} to {ds.time.data[-1]}"
            ds.to_netcdf(savepath + a + output, unlimited_dims="time")
        print(f"{tabs}Finished creating {a + output}.")
    finally:
        ds.close()
dataset.close()
